---
title: "C. Clean"
author: Jeffrey W. Hollister & Luke Winslow
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  rmarkdown::html_vignette:
    toc: true
    fig_width: 6
    fig_height: 6
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteIndexEntry{C. Clean}
  \usepackage[utf8]{inputenc}
---

```{r setup, echo=FALSE}
set.seed(3)
title="C. Clean"
gsIntroR::navigation_array(title)
```


In this third lesson we are going to start working on manipulating and cleaning up our data frames.  We are spending some time on this because, in my experience, most data analysis and statistics classes seem to assume that 95% of the time spent working with data is on the analysis and interpretation of that analysis and little time is spent getting data ready to analyze.  However, in reality, the time spent is flipped with most time spent on cleaning up data and significantly less time on the analysis. We will just be scratching the surface of the many ways you can work with data in R.  We will show the basics of subsetting, merging, modifying, and sumarizing data and our examples will all use Hadley Wickham and Romain Francois' `dplyr` package.  There are many ways to do this type of work in R, many of which are available from base R, but I heard from many (AED colleagues and Hadley himself!) focusing on one way to do this is best, so `dplyr` it is!

Before we jump into the lesson, quick links and lesson goals are:


##Quick Links to Exercises and R code
- [Exercise 1](#exercise-1): Subsetting data with `dplyr`.
- [Exercise 2](#exercise-2): Merging two data frames together.
- [Exercise 3](#exercise-3): Using `dplyr` to modify and summarize data.


##Lesson Goals
- Show and tell on using base R for data manipulation
- Better understand data cleaning through use of `dplyr`
- Use conditional statements to further manipulate data
- Use `merge()` to combine data frames by a common key
- Do some basic reshaping and summarizing data frames
- Know what pipes are and why you might want to use them

##What is `dplyr`? 

The package `dplyr` is a fairly new (2014) package that tries to provide easy tools for the most common data manipulation tasks.  It is built to work directly with data frames. The thinking behind it was largely inspired by the package `plyr` which has been in use for some time but suffered from being slow in some cases. `dplyr` addresses this by porting much of the computation to C++. The result is a fast package that gets a lot done with very little code from you.

An additional feature is the ability to work with data stored directly in an external database.  The benefits of doing this are that the data can be managed natively in a relational database, queries can be conducted on that database, and only the results of the query returned. This addresses a common problem with R in that all operations are conducted in memory and thus the amount of data you can work with is limited by available memory.  The database connections essentially remove that limitation in that you can have a database of many 100s GB, conduct queries on it directly and pull back just what you need for analysis in R.  

There is a lot of great info on `dplyr`.  If you have an interest, I'd encourage you to look more.  The vignettes are particulary good.

- [`dplyr` GitHub repo](https://github.com/hadley/dplyr)
- [CRAN page: vignettes here](http://cran.rstudio.com/web/packages/dplyr/)

##Subsetting in base R
In base R you can use a indexing to select out rows and columns.  You will see this quite often in other people's code, so I want to at least show it to you.  

```{r indexing_examp}
#Create a vector
x <- c(10:19)
x
#Positive indexing returns just the value in the ith place
x[7]
#Negative indexing returns all values except the value in the ith place
x[-3]
#Ranges work too
x[8:10]
#A vector can be used to index
#Can be numeric
x[c(2,6,10)]
#Can be boolean - will repeat the pattern 
x[c(TRUE,FALSE)]
#Can even get fancy
x[x %% 2 == 0]
```

You can also index a data frame or select individual columns of a data frame.  Since a data frame has two dimensions, you need to specify an index for both the row and the column.  You can specify both and get a single value like `data_frame[row,column]`, specify just the row and the get the whole row back like `data_frame[row,]` or get just the column with `data_frame[,column]`. These examples show that. 

For the remainder of this course, I will be using prefabricated datasets from the `smwrData` package. This package is a collection of real hydrologic data that can be loaded into your R workspace and used. It is similar to the stock R datasets (type `data()` into your console to see stock datasets), but with USGS-relevant data. So don't forget to load the package for each new R session (I will remind you at the beginning of each lesson).

```{r data_frame_index}
#Let's use one a data frame from the smwrData package

#Load the package and data frame:
library(smwrData)
data("PugetNitrate")

head(PugetNitrate)
#And grab a specific value
PugetNitrate[1,1]
#A whole column
nitrate_levels <- PugetNitrate[,7]
nitrate_levels
#A row
obs15<-PugetNitrate[15,]
obs15
#Many rows
obs3to7<-PugetNitrate[3:7,]
obs3to7
```

Also remember that data frames have column names.  We can use those too.  Let's try it.

```{r more_data_frame_index}
#First, there are a couple of ways to use the column names
PugetNitrate$wellid
head(PugetNitrate["wellid"])
#Multiple colums
head(PugetNitrate[c("date","nitrate")])
#Now we can combine what we have seen to do some more complex queries
#Get all the data where nitrate concentration is greater than 10
high_nitrate <- PugetNitrate[PugetNitrate$nitrate > 10,]
head(high_nitrate)
#Or maybe we want just the nitrate concentrations for Bedrock geology
bedrock_nitrate <- PugetNitrate$nitrate[PugetNitrate$surfgeo == "BedRock"]
head(bedrock_nitrate)
```


##Data Manipulation in `dplyr`
So, base R can do what you need, but it is a bit complicated and the syntax is a bit dense.  In `dplyr` this can be done with two functions, `select()` and `filter()`.  The code can be a bit more verbose, but it allows you to write code that is much more readable.  Before we start we need to make sure we've got everything installed and loaded.  If you do not have R Version 3.0.2 or greater you will have some problems (i.e. no `dplyr` for you).

```{r real_setup, echo=FALSE, include=FALSE, purl=FALSE}
if(!require("dplyr")){
  install.packages("dplyr")
}
library("dplyr")
```

```{r setup_dplyr,eval=FALSE}
install.packages("dplyr")
library("dplyr")
```

I am going to repeat some of what I showed above on data frames but now with `dplyr`.  This is what we will be using in the exercises.

```{r more_data_frame_dplyr}
#First, select some columns
dplyr_sel <- select(PugetNitrate, date, nitrate, surfgeo)
head(dplyr_sel)
#That's it.  Select one or many columns
#Now select some observations, like before
dplyr_high_nitrate <- filter(PugetNitrate, nitrate > 10)
head(dplyr_high_nitrate)
#Or maybe we want just the bedrock samples
bedrock_nitrate <- filter(PugetNitrate, surfgeo == "BedRock")
head(bedrock_nitrate)
```

Now we have seen how to filter observations and select columns within a data frame. Now I want to add a new column. In dplyr, `mutate()` allows us to add new columns. These can be vectors you are adding or based on expressions applied to existing columns. For instance, we have a column of well depths in meters, but we would like to add a column with well depths in kilometers. 

```{r mutate_example}
#Add a column with well depth in kilometers instead of meters
PugetNitrate_newcolumn <- mutate(PugetNitrate, wellkm = wellmet/1000)
head(PugetNitrate_newcolumn)
```

Let's take a quick detour and learn about how to apply conditional statements to our data cleaning workflows.

## Using if-else statements with dplyr
If you have done any programing in any language, then `if-else` statements are not new to you.  All they do is tell your code how to make decisions, and they come in handy when cleaning up data frames. For instance, you might want to make a new column of data based on some condition applied to an existing column. That's easy with if-else control structures. Here's the basic syntax of an if-else statement:

```
if(some condition){
  do something
} else {
  do something different
}
```

Let's first go through basic if-else structures before using them with data frame manipulation.

```{r if_else_examp}
x <- 2

# logical statement inside of () needs to return ONE logical value - TRUE or FALSE. TRUE means it will enter the following {}, FALSE means it won't.
if(x < 0){
  print("negative")
} 

# you can also specify something to do when the logical statement is FALSE by adding `else`
if(x < 0){
  print("negative")
} else {
  print("positive")
}
```

The key to if-else is using a logical statement that only returns a single TRUE or FALSE, not a vector of trues or falses. The if statement needs to know if it should enter the `{}` or not, and therefore needs a single answer - yes or no. Here are some useful functions that allow you to use vectors in if-else:

```{r if_else_functions}
y <- 1:7

# use "any" if you want to see if at least one of the values meets a condition
any(y > 5)

# use "!any" if you don't want any of the values to meet some condition (e.g. vector can't have negatives)
!any(y < 0) 

# use "all" when every value in a vector must meet a condition
all(y > 5)

# using these in the if-else statement
if(any(y < 0)){
  print("some values are negative")
} 
```

And you can you use multiple `if` statements by stringing them together with `else`

```{r if_else_examp2}
num <- 198

if(num > 0) {
  print("positive")
} else if (num < 0) {
  print("negative")
} else {
  print("zero")
}
```

Now that you have a basic understanding of if-else structures, let's use them with `dplyr` to manipulate a data frame. We want to add a new column if some condition is met, or filter a column if the condition is not met. We can use the `if-else` structure along with `mutate` and `select` from `dplyr` to accomplish this. 

```{r if_else_dplyr}
# if the column "wellkm" (well depth in kilometers) does not exist, we want to add it
if(!'wellkm' %in% names(PugetNitrate)){
  mutate(PugetNitrate, wellkm = wellmet/1000)
} 

# if there are more than 1000 observations, we want to filter out older observations
if(nrow(PugetNitrate) > 1000){
  filter(PugetNitrate, date >= as.Date("1990-01-01"))
}
```

### Optional: the `ifelse` function

Let's learn a new way to apply if-else logic - the function `ifelse`. This function is similar to the previous if-else structure syntax, but the function can only return one value (rather than doing a series of commands within `{}`). See `?ifelse` for more information. Basic syntax for `ifelse()`:

`ifelse(condition, yesValue, noValue)`

Add a new column to `PugetNitrate` that categorizes each Nitrate value as "high" or "low". 

```{r ifelse_dplyr}
#use mutate along with ifelse to add a new column
PugetNitrate_categorized <- mutate(PugetNitrate, nitrate_category = ifelse(nitrate > 1, "high", "low"))
```

## Three ways to string `dplyr` commands together

But what if I wanted to select and filter the same data frame? There are three ways to do this: use intermediate steps, nested functions, or pipes.  With the intermediate steps, you essentially create a temporary data frame and use that as input to the next function.  You can also nest functions (i.e. one function inside of another).  This is handy, but can be difficult to read if too many functions are nested from inside out.  The last option, pipes, are a fairly recent addition to R.  Pipes in the Unix/Linux world are not new and allow you to chain commands together where the output of one command is the input to the next.  This provides a more natural way to read the commands in that they are executed in the way you conceptualize it and make the interpretation of the code a bit easier.  Pipes in R look like `%>%` and are made available via the `magrittr` package, which is installed as part of `dplyr`.  Let's try all three with the same analysis: selecting out a subset of columns but for only a single species.

```{r combine_commands}
#Intermediate data frames
#Select First: note the order of the output, neat too!
dplyr_bedrock_tmp1 <- select(PugetNitrate, surfgeo, date, nitrate)
dplyr_bedrock_tmp <- filter(dplyr_bedrock_tmp1, surfgeo == "BedRock")
head(dplyr_bedrock_tmp)

#Nested function
dplyr_bedrock_nest <- filter(
  select(PugetNitrate, surfgeo, date, nitrate),
  surfgeo == "BedRock")
head(dplyr_bedrock_nest)

#Pipes
dplyr_bedrock_pipe <- PugetNitrate %>% 
  select(surfgeo, date, nitrate) %>%
  filter(surfgeo == "BedRock")
head(dplyr_bedrock_pipe)

# Every function, including head(), can be chained
PugetNitrate %>% 
  select(surfgeo, date, nitrate) %>%
  filter(surfgeo == "BedRock") %>% 
  head()
```

Although we show you the nested and piping methods, we will only use the intermediate data frames method for the remainder of this material. 

```{r Exercise1, echo=FALSE}
```

##Exercise 1
This exercise is going to focus on using what we just covered on `dplyr` to start to clean up a dataset.  Remember to use the stickies: green when you're done, red if you have a problem.

1. If it isn't already open, make sure you have the script we created, "usgs_analysis.R" opened up.
2. Start a new section of code in this script by simply putting in a line or two of comments indicating what it is this set of code does. Our goal for this is to create a new data frame that represents a subset of the observations as well as a subset of the data. 
3. First, we want a new data frame based on the PugetNitrate dataset from the `smwrData` package (don't forget to load your package!). Load the data by executing `data(PugetNitrate)`. 
4. Using dplyr, remove the landuse columns (l10, l20, and l40). Think `select()`. Give the new data frame a new name, so you can distinguish it from your raw data. 
5. Next, we are going to get a subset of the observations. We only want wells where the surficial geology is Alluvium or Fine. Also give this data frame a different name than before.
6. Lastly, add a new column using an `ifelse` function that tells us if the well depth is "shallow" or "deep" based on the well depth column values `wellmet`. Hint: use `summary(PugetNitrate)` to determine what threshold would determine "shallow" vs "deep" based on the data.

##Merging Data
Joining data in `dplyr` is accomplished via the various `x_join()` commands (e.g., `inner_join`, `left_join`, `anti_join`, etc).  These are very SQL-esque so if you speak SQL then these will be pretty easy for you.  If not then they aren't immediately intuitive.  There are also the base functions `rbind()` and `merge()`, but we won't be covering these because they are redundant with the faster, more readable `dplyr` functions.  

We are going to talk about several different ways to do this.  First, let's add some new rows to a data frame.  This is very handy as you might have data collected and entered at one time, and then additional observations made later that need to be added.  So with `rbind()` we can stack two data frames with the same columns to store more observations.  

```{r bind_rows_examp}
#Let's first create a new small example data.frame
bind_rows_df1 <- data.frame(a=1:3, b=c("a","b","c"), c=c(T,T,F), d=rnorm(3))
#Now an example df to add
bind_rows_df2 <- data.frame(a=10:12, b=c("x","y","z"), c=c(F,F,F), d=rnorm(3))
bind_rows_df <- bind_rows(bind_rows_df1, bind_rows_df2)
bind_rows_df
```

Now something to think about.  Could you add a vector as a new row?  Why/Why not? When/When not?

Let's go back to the columns now. There are simple ways to add columns of the same length with observations in the same order to a data frame, but it is very common to have to datasets that are in different orders and have differing numbers of rows.  What we want to do in that case is going to be more of a database type function and join two tables based on a common column.  We can achieve this by using `x_join` functions. So let's contrive another example by creating a dataset to merge to `bind_rows_df` that we created above. We are going to use the `left_join` function from `dplyr`, which keeps all rows from the first (left) data frame. See `?left_join` for more information.

```{r merge_example}
# Contrived data frame
bind_rows_df_merge_me <- data.frame(
  a=c(1,3,10,11,14,6,23), x=rnorm(7), 
  names=c("bob","joe","sue",NA,NA,"jeff",NA))

bind_rows_df_merge <- left_join(bind_rows_df, bind_rows_df_merge_me, by="a")
bind_rows_df_merge
```

```{r Exercise2, echo=FALSE}
```

##Exercise 2
In this exercise we are going to practice merging data. We will be using two datasets from the `smwrData` package.

1. Load `ChoptankFlow` and `ChoptankNH3` into your environment from the `smwrData` package by using `data()`. 
2. Add to your script a line (or more if you need it) to create a new data frame, `Choptank_Flow_NH3`, that is a merge of `ChoptankFlow` and `ChoptankNH3`, but with only lines in `ChoptankNH3` preserved in the output. The column to merge on is the date column (although named differently in each data frame, but they will need to have the same name to merge).  
3. This data frame may have some `NA` values. Add another line to your code and create a data frame that removes all NA values from `Choptank_Flow_NH3`.
4. If that goes quickly, feel free to explore other joins (`inner_join`, `full_join`, etc).

##Modify and Summarize
One area where `dplyr` really shines is in modifying and summarizing. First, we'll look at an example of grouping a data frame and summarizing the data within those groups. We do this with `group_by()` and  `summarize()`. Group PugetNitrate by the surface geology - you won't notice much of change between this new data frame and the original because `group_by` is changing the class of the data frame so that `dplyr` handles it appropriately in the next function. 

```{r group_by_examp}
class(PugetNitrate)

# Group the data frame
PugetNitrate_grouped <- group_by(PugetNitrate, surfgeo)
class(PugetNitrate_grouped)
```

Now we can summarize the data frame by the groups established previously.

```{r summarize_examp}
PugetNitrate_summary <- summarize(PugetNitrate_grouped, mean(nitrate), mean(wellmet))
PugetNitrate_summary
```

There are many other functions in `dplyr` that are useful. Let's run through some examples with `arrange()` and `slice()`.

First `arrange()` will re-order a data frame based on the values in a specified column.  It will take multiple columns and can be in descending or ascending order. Let's try a different `smwrData` data frame this time:  `TNLoads`.  

```{r arrange_example}
data("TNLoads")
head(TNLoads)

#ascending order is default
head(arrange(TNLoads, LOGTN))
#descending
head(arrange(TNLoads, desc(LOGTN)))
#multiple columns: most nitrogen with lowest rainfall at top
head(arrange(TNLoads, desc(LOGTN), MSRAIN))
```

Now `slice()` which accomplishes what we did with the numeric indices before. Remembering back to that, we could grab rows of the data frame with something like `x[3:10,]`.  

```{r slice_example}
#grab rows 3 through 10
slice(TNLoads, 3:10)
```

Lastly, one more function, `rowwise()`, allows us run rowwise, operations. `rowwise()`, like `group_by` will only change the class of the data frame in preparation for the next `dplyr` function. Let's use this to test if the percent landuse observations in `TNLoads` are reasonable (they should total to 100). Landuse columns are PRES (residential), PNON (non-urban), PCOMM (commercial), PIND (industrial).

```{r rowwise_examp}
class(TNLoads)

TNLoads_byrow <- rowwise(TNLoads)
class(TNLoads_byrow)

#Add a column that totals landuse for each observation
landuse_sum <- mutate(TNLoads_byrow, landuse_total = sum(PRES, PNON, PCOMM, PIND))
head(landuse_sum)
```

We now have quite a few tools that we can use to clean and manipulate data in R.  We have barely touched what both base R and `dplyr` are capable of accomplishing, but hopefully you now have some basics to build on. 

Let's practice some of these last functions with other `smwrData` datasets.

```{r Exercise3, echo=FALSE}
```

##Exercise 3

Next, we're going to practice summarizing large datasets. We will use the `MC11_1993` soil temperature dataset from the `smwrdata` package (271 rows, 10 columns). If you complete a step and notice that your neighbor has not, see if you can answer any questions to help them get it done. 

1. Create a new data.frame that gives the maximum reference temperature (`TEMP.REF`) for each month and name it `MC11_1993_max_monthly_ref_temp`. Hint: don't forget about `group_by()`!

2. Now add a new column that is the average soil temperature at each depth (do not include `TEMP.REF`). Then, sort the resulting data.frame in descending order. Name this new data.frame `MC11_1993_daily_avg_temp`. Hint: use `rowwise` to compute at each depth.

3. Challenge: Find the average and minimum temperatures (for each month) at depths of 0.5, 1.5, and 2.5 using `summarize_each`. 


```{r echo=FALSE}
gsIntroR::navigation_array(title)
```

